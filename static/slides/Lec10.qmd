---
title: "scikit-learn"
subtitle: "Lecture 10"
author: "Dr. Colin Rundel"
footer: "Sta 663 - Spring 2025"
format:
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    self-contained: true
execute: 
  echo: true
  warning: true
---

```{python setup}
#| include: false
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

plt.rcParams['figure.dpi'] = 200
```

```{r r_setup}
#| include: false
knitr::opts_chunk$set(
  fig.align="center",
  cache=FALSE
)

local({
  hook_err_old <- knitr::knit_hooks$get("error")  # save the old hook
  knitr::knit_hooks$set(error = function(x, options) {
    # now do whatever you want to do with x, and pass
    # the new x to the old hook
    x = sub("## \n## Detailed traceback:\n.*$", "", x)
    x = sub("Error in py_call_impl\\(.*?\\)\\: ", "", x)
    hook_err_old(x, options)
  })
  
  hook_warn_old <- knitr::knit_hooks$get("warning")  # save the old hook
  knitr::knit_hooks$set(warning = function(x, options) {
    x = sub("<string>:1: ", "", x)
    hook_warn_old(x, options)
  })
})
```


## scikit-learn

> Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.
>
> * Simple and efficient tools for predictive data analysis
> * Accessible to everybody, and reusable in various contexts
> * Built on NumPy, SciPy, and matplotlib
> * Open source, commercially usable - BSD license


::: {.aside}
This is one of several other "scikits" (e.g. scikit-image) which are scientific toolboxes built on top of scipy. For a more complete list see [here](https://pypi.org/search/?q=scikit).
:::

## Submodules

The `sklearn` package contains a large number of submodules which are specialized for different tasks / models,

:::: {.columns .tiny}
::: {.column width='50%'}
- `sklearn.base` - Base classes and utility functions
- `sklearn.calibration` - Probability Calibration
- `sklearn.cluster` - Clustering
- `sklearn.compose` - Composite Estimators
- `sklearn.covariance` - Covariance Estimators
- `sklearn.cross_decomposition` - Cross decomposition
- `sklearn.datasets` - Datasets
- `sklearn.decomposition` - Matrix Decomposition
- `sklearn.discriminant_analysis` - Discriminant Analysis
- `sklearn.ensemble` - Ensemble Methods
- `sklearn.exceptions` - Exceptions and warnings
- `sklearn.experimental` - Experimental
- `sklearn.feature_extraction` - Feature Extraction
- `sklearn.feature_selection` - Feature Selection
- `sklearn.gaussian_process` - Gaussian Processes
- `sklearn.impute` - Impute
- `sklearn.inspection` - Inspection
- `sklearn.isotonic` - Isotonic regression
- `sklearn.kernel_approximation` - Kernel Approximation
:::

::: {.column width='50%'}
- `sklearn.kernel_ridge` - Kernel Ridge Regression
- `sklearn.linear_model` - Linear Models
- `sklearn.manifold` - Manifold Learning
- `sklearn.metrics` - Metrics
- `sklearn.mixture` - Gaussian Mixture Models
- `sklearn.model_selection` - Model Selection
- `sklearn.multiclass` - Multiclass classification
- `sklearn.multioutput` - Multioutput regression and classification
- `sklearn.naive_bayes` - Naive Bayes
- `sklearn.neighbors` - Nearest Neighbors
- `sklearn.neural_network` - Neural network models
- `sklearn.pipeline` - Pipeline
- `sklearn.preprocessing` - Preprocessing and Normalization
- `sklearn.random_projection` - Random projection
- `sklearn.semi_supervised` - Semi-Supervised Learning
- `sklearn.svm` - Support Vector Machines
- `sklearn.tree` - Decision Trees
- `sklearn.utils` - Utilities
:::
::::


# Model Fitting

## Sample data

To begin, we will examine a simple data set on the size and weight of a number of books. The goal is to model the weight of a book using some combination of the other features in the data. 

:::: {.columns}
::: {.column width='50%'}
The included columns are:

* `volume` - book volumes in cubic centimeters

* `weight` - book weights in grams

* `cover` - a categorical variable with levels `"hb"` hardback, `"pb"` paperback
:::

::: {.column width='50%' .xsmall}
```{python}
books = pd.read_csv("data/daag_books.csv"); books
```
:::
::::

::: {.aside}
These data come from the `allbacks` data set from the `DAAG` package in R
:::

##

::: {.small}
```{python}
#| out-width: 50%
g = sns.relplot(data=books, x="volume", y="weight", hue="cover")
```
:::

## Linear regression

scikit-learn uses an object oriented system for implementing the various modeling approaches, the class for `LinearRegression` is part of the `linear_model` submodule.

::: {.small}
```{python}
from sklearn.linear_model import LinearRegression 
```
:::

. . .

Each modeling class needs to be constructed (potentially with options) and then the resulting object will provide attributes and methods. 

. . .

:::: {.columns .small}
::: {.column width='50%'}
```{python}
lm = LinearRegression()

m = lm.fit(
  X = books[["volume"]],
  y = books.weight
)
```

```{python}
m.coef_
m.intercept_
```
:::

::: {.column width='50%' .fragment}
Note `lm` and `m` are labels for the same underlying `LinearRegression` object,

```{python}
lm.coef_
lm.intercept_
```
:::
::::


## A couple of considerations

When fitting a model, scikit-learn expects `X` to be a 2d array-like object (e.g. a `np.array` or `pd.DataFrame`), so it will not accept objects like a `pd.Series` or 1d `np.array`.

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
#| error: true
lm.fit(
  X = books.volume,
  y = books.weight
)
```
:::

::: {.column width='50%'}
```{python}
#| error: true
lm.fit(
  X = np.array(books.volume),
  y = books.weight
)
```
:::
::::

. . .

:::: {.xsmall}
```{python}
#| eval: false
lm.fit(
  X = np.array(books.volume).reshape(-1,1),
  y = books.weight
)
```
:::



## Model parameters

Depending on the model being used, there will be a number of parameters that can be configured when creating the model object or via the `set_params()` method.

::: {.small}
```{python}
lm.get_params()
```
:::

. . .

::: {.small}
```{python}
lm.set_params(fit_intercept = False)
```
:::

. . .

::: {.small}
```{python}
lm = lm.fit(X = books[["volume"]], y = books.weight)
lm.intercept_
lm.coef_
```
:::

## Model prediction

Once the model coefficients have been fit, it is possible to predict from the model via the `predict()` method, this method requires a matrix-like `X` as input and in the case of `LinearRegression` returns an array of predicted y values. 

::: {.small}
```{python}
lm.predict(X = books[["volume"]])
```

```{python}
books = books.assign(
  weight_lm_pred = lambda x: lm.predict(X = x[["volume"]])
)
books
```
:::

##

::: {.small}
```{python}
plt.figure()
sns.scatterplot(data=books, x="volume", y="weight", hue="cover")
sns.lineplot(data=books, x="volume", y="weight_lm_pred", color="c")
plt.show()
```
:::


## Residuals?

There is no built in functionality for calculating residuals, so this needs to be done by hand.

::: {.small}
```{python}
books["resid_lm_pred"] = books["weight"] - books["weight_lm_pred"]
```

```{python}
#| out-width: 66%
plt.figure(layout="constrained")
ax = sns.scatterplot(data=books, x="volume", y="resid_lm_pred", hue="cover")
ax.axhline(c="k", ls="--", lw=1)
plt.show()
```
:::


## Categorical variables?

Scikit-learn expects that the model matrix be numeric before fitting,

::: {.small}
```{python error=TRUE}
lm = lm.fit(
  X = books[["volume", "cover"]],
  y = books.weight
)
```
:::

. . .

the solution here is to dummy code the categorical variables - this can be done with pandas via `pd.get_dummies()` or with a scikit-learn preprocessor.

::: {.small}
```{python}
pd.get_dummies(books[["volume", "cover"]])
```
:::


## Dummy coded model



::: {.small}
```{python}
lm = LinearRegression().fit(
  X = pd.get_dummies(books[["volume", "cover"]]),
  y = books.weight
)
```

```{python}
lm.intercept_
lm.coef_
```
:::

Do the above results look reasonable? What went wrong?


## Quick comparison with R

::: {.xsmall}
```{r}
d = read.csv('data/daag_books.csv')
d['cover_hb'] = ifelse(d$cover == "hb", 1, 0)
d['cover_pb'] = ifelse(d$cover == "pb", 1, 0)
lm = lm(weight~volume+cover_hb+cover_pb, data=d)
summary(lm)
```
:::


## Avoiding co-linearity

:::: {.columns .small}
::: {.column width='50%'}
```{python}
lm1 = LinearRegression(
  fit_intercept = False
).fit(
  X = pd.get_dummies(
    books[["volume", "cover"]]
  ),
  y = books.weight
)
```
:::

::: {.column width='50%'}
```{python}
lm2 = LinearRegression(
  fit_intercept = True
).fit(
  X = pd.get_dummies(
    books[["volume", "cover"]], 
    drop_first=True
  ),
  y = books.weight
)
```
:::
::::

:::: {.columns .small}
::: {.column width='50%'}
```{python}
lm1.intercept_
lm1.coef_
lm1.feature_names_in_
```
:::

::: {.column width='50%'}
```{python}
lm2.intercept_
lm2.coef_
lm2.feature_names_in_
```
:::
::::


# Preprocessors

## Preprocessors

These are a collection of transformer classes present in the `sklearn.preprocessing` submodule that are designed to help with the preparation of raw feature data into quantities more suitable for downstream modeling tools.

Like the modeling classes, they have an object oriented design that shares a common interface (methods and attributes) for bringing in data, transforming it, and returning it.


## OneHotEncoder

For dummy coding we can use the `OneHotEncoder` preprocessor, the default is to use one hot encoding but standard dummy coding can be achieved via the `drop` parameter.

::: {.xsmall}
```{python}
from sklearn.preprocessing import OneHotEncoder
```
:::

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
enc = OneHotEncoder(sparse_output=False)
enc.fit(X = books[["cover"]])
enc.transform(X = books[["cover"]])
```
:::

::: {.column width='50%' .fragment}
```{python}
enc = OneHotEncoder(sparse_output=False, drop="first")
enc.fit_transform(X = books[["cover"]])
```
:::
::::


## Other useful bits

```{python, include=FALSE}
enc = OneHotEncoder(sparse_output=False)
enc.fit(X = books[["cover"]])
```

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
enc.get_feature_names_out()
f = enc.transform(X = books[["cover"]])
f
```
:::

::: {.column width='50%' .fragment}
```{python}
enc.inverse_transform(f)
```
:::
::::



## A cautionary note

Unlike `pd.get_dummies()` it is not safe to use `OneHotEncoder` with both numerical and categorical features, as the former will also be transformed.

::: {.small}
```{python}
enc = OneHotEncoder(sparse_output=False)
X = enc.fit_transform(X = books[["volume", "cover"]])
pd.DataFrame(data=X, columns = enc.get_feature_names_out())
```
:::


## Putting it together

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
cover = OneHotEncoder(
  sparse_output=False
).fit_transform(
  books[["cover"]]
)
X = np.c_[books.volume, cover]

lm2 = LinearRegression(
  fit_intercept=False
).fit(
  X = X,
  y = books.weight
)

lm2.coef_
```
:::

::: {.column width='50%' .fragment}
```{python}
books["weight_lm2_pred"] = lm2.predict(X=X)
books.drop(
  ["weight_lm_pred", "resid_lm_pred"], 
  axis=1
)
```
:::
::::

::: {.aside}
We'll see a more elegant way of doing this in the near future
:::

## Model fit

```{python}
#| echo: false
plt.figure()
sns.scatterplot(data=books, x="volume", y="weight", hue="cover")
sns.lineplot(data=books, x="volume", y="weight_lm2_pred", hue="cover")
plt.show()
```

## Model residuals

```{python}
#| echo: false
books["resid_lm2_pred"] = books["weight"] - books["weight_lm2_pred"]

plt.figure(layout="constrained")
ax = sns.scatterplot(data=books, x="volume", y="resid_lm2_pred", hue="cover")
ax.axhline(c="k", ls="--", lw=1)
plt.show()
```


## Model performance

Scikit-learn comes with a number of builtin functions for measuring model performance in the `sklearn.metrics` submodule - these are generally just functions that take the vectors `y_true` and `y_pred` and return a scalar score.

::: {.xsmall}
```{python}
import sklearn.metrics as metrics 
```
:::

. . .

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
metrics.r2_score(books.weight, books.weight_lm_pred)
metrics.mean_squared_error(
  books.weight, books.weight_lm_pred
)
metrics.root_mean_squared_error(
  books.weight, books.weight_lm_pred
)
```
:::

::: {.column width='50%'}
```{python}
metrics.r2_score(books.weight, books.weight_lm2_pred)
metrics.mean_squared_error(
  books.weight, books.weight_lm2_pred
) 
metrics.root_mean_squared_error(
  books.weight, books.weight_lm2_pred
)
```
:::
::::

::: {.aside}
See [API Docs](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for a list of available metrics
:::


## Exercise 1

Create and fit a model for the `books` data that includes an interaction effect between `volume` and `cover`. 

You will need to do this manually with `pd.getdummies()` and some additional data munging.

The data can be read into pandas with,
```{python}
#| eval: false
books = pd.read_csv(
  "https://sta663-sp25.github.io/slides/data/daag_books.csv"
)
```



# Other transformers

## Polynomial regression

We will now look at another flavor of regression model, that involves preprocessing and a hyperparameter - namely polynomial regression.

```{python, out.width="40%"}
df = pd.read_csv("data/gp.csv")
sns.relplot(data=df, x="x", y="y")
```


## By hand

It is certainly possible to construct the necessary model matrix by hand (or even use a function to automate the process), but this is less then desirable generally - particularly if we want to do anything fancy (e.g. cross validation)

:::: {.columns .small}
::: {.column width='50%'}
```{python}
X = np.c_[
    np.ones(df.shape[0]),
    df.x,
    df.x**2,
    df.x**3
]

plm = LinearRegression(
  fit_intercept = False
).fit(
  X=X, y=df.y
)

plm.coef_
```
:::

::: {.column width='50%' .fragment}
```{python}
df["y_pred"] = plm.predict(X=X)

plt.figure(layout="constrained")
sns.scatterplot(data=df, x="x", y="y")
sns.lineplot(data=df, x="x", y="y_pred", color="k")
plt.show()
```
:::
::::

##

::: {.columns .xsmall}
```{python}
X = np.c_[
    np.ones(df.shape[0]), df.x,
    df.x**2, df.x**3,
    df.x**4, df.x**5
]

plm = LinearRegression(
  fit_intercept = False
).fit(
  X=X, y=df.y
)
df["y_pred"] = plm.predict(X=X)
```
:::


```{python}
#| echo: false
plt.figure(layout="constrained")
sns.scatterplot(data=df, x="x", y="y")
sns.lineplot(data=df, x="x", y="y_pred", color="k")
plt.show()
```


## PolynomialFeatures

::: {.small}
This is another transformer class from `sklearn.preprocessing` that simplifies the process of constructing polynormial features for your model matrix. Usage is similar to that of `OneHotEncoder`.
:::

::: {.xsmall}
```{python}
from sklearn.preprocessing import PolynomialFeatures
X = np.array(range(6)).reshape(-1,1)
```
:::

:::: {.columns .small}
::: {.column width='50%'}
```{python}
pf = PolynomialFeatures(degree=3)
pf = pf.fit(X)
pf.transform(X)
pf.get_feature_names_out()
```
:::

::: {.column width='50%' .fragment}
```{python}
pf = PolynomialFeatures(
  degree=2, include_bias=False
)
pf.fit_transform(X)
pf.get_feature_names_out()
```
:::
::::


## Interactions

If the feature matrix `X` has more than one column than `PolynomialFeatures` transformer will include interaction terms with total degree up to `degree`.

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
X.reshape(-1, 2)

pf = PolynomialFeatures(
  degree=2, include_bias=False
)
pf.fit_transform(
  X.reshape(-1, 2)
)
pf.get_feature_names_out()
```
:::

::: {.column width='50%' .fragment}
```{python}
X.reshape(-1, 3)

pf = PolynomialFeatures(
  degree=2, include_bias=False
)
pf.fit_transform(
  X.reshape(-1, 3)
)
pf.get_feature_names_out()
```
:::
::::


## Modeling with PolynomialFeatures

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
def poly_model(X, y, degree):
  X  = PolynomialFeatures(
    degree=degree, include_bias=False
  ).fit_transform(
    X=X
  )
  y_pred = LinearRegression(
  ).fit(
    X=X, y=y
  ).predict(
    X
  )
  return metrics.root_mean_squared_error(y, y_pred)
```

```{python}
poly_model(X = df[["x"]], y = df.y, degree = 2)
poly_model(X = df[["x"]], y = df.y, degree = 3)
```
:::

::: {.column width='50%' .fragment}
```{python}
#| out-width: 66%
degrees = range(1,10)
rmses = [
  poly_model(X=df[["x"]], y=df.y, degree=d) 
  for d in degrees
]
g = sns.relplot(x=degrees, y=rmses)
```
:::
::::

##

```{python}
#| echo: false
res = df.copy().drop("y_pred", axis=1)
for d in range(1,10):
  X  = PolynomialFeatures(
    degree=d, include_bias=False
  ).fit_transform(
    X=res[["x"]]
  )
  res[str(d)] = LinearRegression().fit(X=X, y=res.y).predict(X)

g = sns.relplot(
  data = res.melt(id_vars=["x","y"], var_name="degree"),
  x = "x", y="value", col = "degree",
  col_wrap=3, kind="line", color="k"
)

[ ax.scatter(res.x, res.y, alpha=0.3)  for ax in g.axes ]
```



# Pipelines


## Pipelines

You may have noticed that `PolynomialFeatures` takes a model matrix as input and returns a new model matrix as output which is then used as the input for `LinearRegression`. This is not an accident, and by structuring the library in this way sklearn is designed to enable the connection of these steps together, into what sklearn calls a *pipeline*.

::: {.columns .xsmall}
::: {.column}
```{python}
from sklearn.pipeline import make_pipeline

p = make_pipeline(
  PolynomialFeatures(degree=4),
  LinearRegression()
)
```
:::
::: {.column}
```{python}
p
```
:::
:::


## Using Pipelines

Once constructed, this object can be used just like our previous `LinearRegression` model (i.e. fit to our data and then used for prediction)

::: {.small}
```{python}
p = p.fit(X = df[["x"]], y = df.y)
p.predict(X = df[["x"]])
```
:::

##

::: {.small}
```{python}
plt.figure(layout="constrained")
sns.scatterplot(data=df, x="x", y="y")
sns.lineplot(x=df.x, y=p.predict(X = df[["x"]]), color="k")
plt.show()
```
:::


## Model coefficients (or other attributes)

The attributes of pipeline steps are not directly accessible, but can be accessed via the `steps` or `named_steps` attributes,

::: {.small}
```{python, error=TRUE}
p.coef_
```
:::

. . .

::: {.small}
```{python}
p.steps
p.steps[1][1].coef_
p.named_steps["linearregression"].intercept_
```
:::

## Other useful bits

::: {.small}
```{python}
p.steps[0][1].get_feature_names_out()
p.steps[1][1].get_params()
```
:::



Anyone notice a problem?

. . .

::: {.small}
```{python}
p.steps[1][1].rank_
p.steps[1][1].n_features_in_
```
:::




## What about step parameters?

By accessing each step we can adjust their parameters (via `set_params()`),

::: {.small}
```{python}
p.named_steps["linearregression"].get_params()
```
:::

. . .

::: {.columns .xsmall}
::: {.column}
```{python}
#| results: false
p.named_steps["linearregression"].set_params(
  fit_intercept=False
)
p.fit(X = df[["x"]], y = df.y)
```
:::
::: {.column .fragment}
```{python}
p.named_steps["linearregression"].intercept_
p.named_steps["linearregression"].coef_
```
:::
:::


## Pipeline parameter names

These parameters can also be directly accessed at the pipeline level, names are constructed as step name + `__` + parameter name:

::: {.xsmall}
```{python}
p.get_params()
p.set_params(
  linearregression__fit_intercept=True, 
  polynomialfeatures__include_bias=False
)
```
:::

##

::: {.xsmall}
```{python}
p.fit(X = df[["x"]], y = df.y)
p.named_steps["polynomialfeatures"].get_feature_names_out()
p.named_steps["linearregression"].intercept_
p.named_steps["linearregression"].coef_
```
:::


# Column Transformers

```{python}
#| include: false
books = pd.read_csv("data/daag_books.csv"); books
```

## Column Transformers

Are a tool for selectively applying transformer(s) to column(s) of an array or DataFrame, they function in a way that is similar to a pipeline and similarly have a `make_` helper function.

::: {.xsmall}
```{python}
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
```
:::

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
ct = make_column_transformer(
  (StandardScaler(), ["volume"]),
  (OneHotEncoder(), ["cover"]),
).fit(
  books
)
ct.get_feature_names_out()
```
:::

::: {.column width='50%'}
```{python}
ct.transform(books)
```
:::
::::


## Keeping or dropping other columns

One addition important argument is `remainder` which determines what happens to unspecified columns. The default is `"drop"` which is why `weight` was removed, the alternative is `"passthrough"` which retains untransformed columns.

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
ct = make_column_transformer(
  (StandardScaler(), ["volume"]),
  (OneHotEncoder(), ["cover"]),
  remainder = "passthrough"
).fit(
  books
)
```
:::

::: {.column width='50%'}
```{python}
ct.get_feature_names_out()
```
:::
::::

::: {.xsmall}
```{python}
ct.transform(books)
```
:::


## Column selection

One lingering issue with the above approach is that we've had to hard code the column names (or use indexes). Often we want to select columns based on their dtype (e.g. categorical vs numerical) this can be done via pandas or sklearn,

::: {.xsmall}
```{python}
from sklearn.compose import make_column_selector
```
:::

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
ct = make_column_transformer(
  ( StandardScaler(), 
    make_column_selector(
      dtype_include=np.number
    )
  ),
  ( OneHotEncoder(), 
    make_column_selector(
      dtype_include=[object, bool]
    )
  )
)
```
:::

::: {.column width='50%'}
```{python}
ct = make_column_transformer(
  ( StandardScaler(), 
    books.select_dtypes(
      include=['number']
    ).columns
  ),
  ( OneHotEncoder(), 
    books.select_dtypes(
      include=['object']
    ).columns
  )
)
```
:::
::::

::: {.aside}
`make_column_selector()` also supports selecting via `pattern` or excluding via `dtype_exclude`
:::

##

:::: {.columns .xsmall}
::: {.column width='50%'}
```{python}
ct.fit_transform(books)
ct.get_feature_names_out()
```
:::

::: {.column width='50%'}
```{python}
ct.fit_transform(books)
ct.get_feature_names_out()
```
:::
::::
