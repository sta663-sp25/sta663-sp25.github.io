---
title: "PyMC + ArviZ"
subtitle: "Lecture 19"
author: "Dr. Colin Rundel"
footer: "Sta 663 - Spring 2023"
format:
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    self-contained: true
execute: 
  echo: true
---

```{python setup}
#| include: false
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import scipy

import pymc as pm
import arviz as az

plt.rcParams['figure.dpi'] = 200

np.set_printoptions(
  linewidth=80,
  precision = 5, suppress=True
  #formatter=dict(float=lambda x: "%.5g" % x)
)

pd.set_option("display.width", 150)
pd.set_option("display.max_columns", 10)
pd.set_option("display.precision", 6)
```

```{r r_setup}
#| include: false

knitr::opts_chunk$set(
  fig.align="center",
  cache=FALSE
)

library(lme4)

local({
  hook_err_old <- knitr::knit_hooks$get("error")  # save the old hook
  knitr::knit_hooks$set(error = function(x, options) {
    # now do whatever you want to do with x, and pass
    # the new x to the old hook
    x = sub("## \n## Detailed traceback:\n.*$", "", x)
    x = sub("Error in py_call_impl\\(.*?\\)\\: ", "", x)
    #x = stringr::str_wrap(x, width = 100)
    hook_err_old(x, options)
  })
  
  hook_warn_old <- knitr::knit_hooks$get("warning")  # save the old hook
  knitr::knit_hooks$set(warning = function(x, options) {
    x = sub("<string>:1: ", "", x)
    #x = stringr::str_wrap(x, width = 100)
    hook_warn_old(x, options)
  })
  
  hook_msg_old <- knitr::knit_hooks$get("output")  # save the old hook
  knitr::knit_hooks$set(output = function(x, options) {
    x = stringr::str_replace(x, "(## ).* ([A-Za-z]+Warning:)", "\\1\\2")
    x = stringr::str_split(x, "\n")[[1]]
    #x = stringr::str_wrap(x, width = 120, exdent = 3)
    x = stringr::str_remove_all(x, "\r")
    x = stringi::stri_wrap(x, width=120, exdent = 3, normalize=FALSE)
    x = paste(x, collapse="\n")
    
    #x = stringr::str_wrap(x, width = 100)
    hook_msg_old(x, options)
  })
})
```

## pymc

::: {.small}

> PyMC is a probabilistic programming library for Python that allows users to build Bayesian models with a simple Python API and fit them using Markov chain Monte Carlo (MCMC) methods.
>
> * **Modern** - Includes state-of-the-art inference algorithms, including MCMC (NUTS) and variational inference (ADVI).
>
> * **User friendly** - Write your models using friendly Python syntax. Learn Bayesian modeling from the many example notebooks.
>
> * **Fast** - Uses Aesara as its computational backend to compile to C and JAX, run your models on the GPU, and benefit from complex graph-optimizations.
>
> * **Batteries included** - Includes probability distributions, Gaussian processes, ABC, SMC and much more. It integrates nicely with ArviZ for visualizations and diagnostics, as well as Bambi for high-level mixed-effect models.
> 
> * **Community focused** - Ask questions on discourse, join MeetUp events, follow us on Twitter, and start contributing.
:::

<br/>

```{python}
import pymc as pm
```


## ArviZ

::: {.small}
> ArviZ is a Python package for exploratory analysis of Bayesian models. Includes functions for posterior analysis, data storage, sample diagnostics, model checking, and comparison.
>
> * **Interoperability** - Integrates with all major probabilistic programming libraries: PyMC, CmdStanPy, PyStan, Pyro, NumPyro, and emcee.
>
> * **Large Suite of Visualizations** - Provides over 25 plotting functions for all parts of Bayesian workflow: visualizing distributions, diagnostics, and model checking. See the gallery for examples.
>
> * **State of the Art Diagnostics** - Latest published diagnostics and statistics are implemented, tested and distributed with ArviZ.
>
> * **Flexible Model Comparison** - Includes functions for comparing models with information criteria, and cross validation (both approximate and brute force).
> 
> * **Built for Collaboration** - Designed for flexible cross-language serialization using netCDF or Zarr formats. ArviZ also has a Julia version that uses the same data schema.
>
> * **Labeled Data** - Builds on top of xarray to work with labeled dimensions and coordinates.
:::

<br/>

```{python}
import arviz as az
```


## Some history

![](imgs/pymc_versions.webp){fig-align="center" width="75%"}

::: {.aside}
Graphic by [Ravin Kumar](https://twitter.com/canyon289) from [PyMC 4.0 Release Announcement](https://www.pymc.io/blog/v4_announcement.html#v4_announcement)
:::

## Model basics

All models are derived from the `Model()` class, unlike what we have seen previously PyMC makes heavy use of Python's context manager using the `with` statement to add model components to a model.

::: {.small}
```{python}
with pm.Model() as norm:
  x = pm.Normal("x", mu=0, sigma=1)
```
:::

. . .

::: {.small}
```{python}
#| error: true
x = pm.Normal("x", mu=0, sigma=1)
```
:::

. . .

Note that `with` blocks do not have their own scope - so variables defined inside are added to the parent scope (becareful about overwriting other variables).

::: {.small}
```{python}
x
type(x)
```
:::


## Random Variables

`pm.Normal()` is an example of a PyMC distribution, which are used to construct models, these are implemented using the `TensorVariable` class which is used for all of the builtin distributions (and can be used to create custom distributions). Generally you will not be interacting with these objects directly, but with that said some useful methods and attributes:

::: {.small}
```{python}
type(norm.x)
norm.x.owner.op
pm.draw(norm.x)
```
:::


## Standalone RVs

If you really want to construct a `TensorVariable` outside of a model this can be done via the `dist` method for each distribution.

```{python}
z = pm.Normal.dist(mu=1, sigma=2, shape=[2,3])
z
pm.draw(z)
```

## Modifying models

Because of this construction it is possible to add additional components to an existing (named) model via subsequent `with` statements (only the first needs `pm.Model()`)

```{python}
with norm:
  y = pm.Normal("y", mu=x, sigma=1, shape=3)
```

. . .

```{python}
norm.basic_RVs
```


## Variable heirarchy

Note that we defined $y|x \sim \mathcal{N}(x, 1)$, so what is happening when we use `pm.draw(norm.y)`?

```{python}
#| error: true
pm.draw(norm.y)
```

. . .

:::: {.columns .small}
::: {.column width='50%'}
```{python}
#| error: true
obs = pm.draw(norm.y, draws=1000); obs
```
:::

::: {.column width='50%' .fragment}
```{python}
#| error: true
np.mean(obs)
np.var(obs)
np.std(obs)
```
:::
::::

. . .

Each time we ask for a draw from `y`, PyMC is first drawing from `x` for us.


## Beta-Binomial model

We will now build a basic model where we know what the solution should look like and compare the results.

::: {.small}
```{python}
with pm.Model() as beta_binom:
  p = pm.Beta("p", alpha=10, beta=10)
  x = pm.Binomial("x", n=20, p=p, observed=5)
```
:::

. . .

In order to sample from the posterior we add a call to `sample()` within the model context.

::: {.small}
```{python}
with beta_binom:
  trace = pm.sample(random_seed=1234, progressbar=False)
```
:::

## `pm.sample()` results

```{python}
print(trace)
print(type(trace))
```

## Xarray - N-D labeled arrays and datasets in Python

> Xarray (formerly xray) is an open source project and Python package that makes working with labelled multi-dimensional arrays simple, efficient, and fun! 
>
> Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, which allows for a more intuitive, more concise, and less error-prone developer experience. The package includes a large and growing library of domain-agnostic functions for advanced analytics and visualization with these data structures.
>
> Xarray is inspired by and borrows heavily from pandas, the popular data analysis package focused on labelled tabular data. It is particularly tailored to working with netCDF files, which were the source of xarrayâ€™s data model, and integrates tightly with dask for parallel computing.

::: {.aside}
See [here](https://arviz-devs.github.io/arviz/getting_started/XarrayforArviZ.html) for more details on xarray
:::

## Digging into `trace`

::: {.small}
```{python}
print(trace.posterior)
```
:::

. . .

::: {.small}
```{python}
print(trace.posterior["p"].shape)
print(trace.sel(chain=0).posterior["p"].shape)
print(trace.sel(draw=slice(500, None, 10)).posterior["p"].shape)
```
:::


## As DataFrame

Posterior values, or subsets, can be converted to DataFrames via the `to_dataframe()` method

:::: {.columns .small}
::: {.column width='50%'}
```{python}
trace.posterior.to_dataframe()
```
:::

::: {.column width='50%'}
```{python}
trace.posterior["p"][0,:].to_dataframe()
```
:::
::::

## Traceplot

::: {.small}
```{python}
ax = az.plot_trace(trace)
plt.show()
```
:::

## Posterior plot

::: {.small}
```{python}
ax = az.plot_posterior(trace, ref_val=[15/40])
plt.show()
```
:::

## PyMC vs Theoretical

::: {.small}
```{python}
p = np.linspace(0, 1, 100)
post_beta = scipy.stats.beta.pdf(p,15,25)
ax = az.plot_posterior(trace, hdi_prob="hide", point_estimate=None)
plt.plot(p, post_beta, "-k", alpha=0.5, label="Theoretical")
plt.legend(['PyMC NUTS', 'Theoretical'])
plt.show()
```
:::


## Autocorrelation plots

::: {.small}
```{python}
ax = az.plot_autocorr(trace, grid=(2,2), max_lag=50)
plt.show()
```
:::

## Forest plots

::: {.small}
```{python}
ax = az.plot_forest(trace)
plt.show()
```
:::

## Other useful diagnostics

Standard MCMC diagnostic statistics are available via `summary()` from ArviZ

::: {.small}
```{python}
az.summary(trace)
```
:::

. . .

individual methods are available for each statistics,

:::: {.columns .small}
::: {.column width='50%'}
```{python}
print(az.ess(trace, method="bulk"))
print(az.ess(trace, method="tail"))
```
:::

::: {.column width='50%'}
```{python}
print(az.rhat(trace))
print(az.mcse(trace))
```
:::
::::


## Demo 1 - Linear regression

Given the below data, we want to fit a linear regression model to the following synthetic data,

```{python}
np.random.seed(1234)
n = 11
m = 6
b = 2
x = np.linspace(0, 1, n)
y = m*x + b + np.random.randn(n)
```

```{python}
#| echo: false
plt.figure(layout="constrained")
plt.scatter(x, y, s=30, label='data')
plt.plot(x, 6*x + 2, label='true regression line', lw=3., c='red')
plt.legend(loc='best')
plt.show()
```

## Model

```{python}
with pm.Model() as lm:
  m = pm.Normal('m', mu=0, sigma=50)
  b = pm.Normal('b', mu=0, sigma=50)
  sigma = pm.HalfNormal('sigma', sigma=5)
  
  pm.Normal('y', mu=m*x + b, sigma=sigma, observed=y)
  
  trace = pm.sample(progressbar=False, random_seed=1234)
```

## Posterior summary

::: {.medium}
```{python}
az.summary(trace)
```
:::


## Trace plots

```{python}
ax = az.plot_trace(trace)
plt.show()
```

## Posterior plots

```{python}
ax = az.plot_posterior(trace, ref_val=[6,2,1], grid=(1,3))
plt.show()
```


## Regression line posterior draws

::: {.small}
```{python}
#| out-width: 50%
plt.scatter(x, y, s=30, label='data')

post_m = trace.posterior['m'].sel(chain=0, draw=slice(0,None,10))
post_b = trace.posterior['b'].sel(chain=0, draw=slice(0,None,10))

plt.figure(layout="constrained")
plt.scatter(x, y, s=30, label='data')
for m, b in zip(post_m.values, post_b.values):
    plt.plot(x, m*x + b, c='gray', alpha=0.1)
plt.plot(x, 6*x + 2, label='true regression line', lw=3., c='red')
plt.legend(loc='best')
plt.show()
```
:::


## Posterior predictive draws

Draws for observed variables can also be generated (posterior predictive draws) via the `sample_posterior_predictive()` method.

::: {.small}
```{python}
with lm:
  pp = pm.sample_posterior_predictive(trace, progressbar=False)

pp
pp.posterior_predictive
```
:::

## Plotting the posterior predictive distribution

```{python}
az.plot_ppc(pp, num_pp_samples=500)
plt.show()
```

## PP draws

::: {.small}
```{python}
plt.figure(layout="constrained")
plt.scatter(x, y, s=30, label='data')
plt.plot(x, pp.posterior_predictive['y'].sel(chain=0).T, c="grey", alpha=0.01)
plt.plot(x, np.mean(pp.posterior_predictive['y'].sel(chain=0).T, axis=1), c='red', label="PP mean")
plt.legend()
plt.show()
```
:::

## PP HDI

::: {.small}
```{python}
plt.figure(layout="constrained")
plt.scatter(x, y, s=30, label='data')
plt.plot(x, np.mean(pp.posterior_predictive['y'].sel(chain=0).T, axis=1), c='red', label="PP mean")
az.plot_hdi(x, pp.posterior_predictive['y'])
plt.legend()
plt.show()
```
:::


## Model revision

::: {.small}
```{python}
with pm.Model() as lm2:
  m = pm.Normal('m', mu=0, sigma=50)
  b = pm.Normal('b', mu=0, sigma=50)
  sigma = pm.HalfNormal('sigma', sigma=5)
  
  y_hat = pm.Deterministic("y_hat", m*x + b)
  
  pm.Normal('y', mu=y_hat, sigma=sigma, observed=y)
  
  trace = pm.sample(random_seed=1234, progressbar=False)
  pp = pm.sample_posterior_predictive(trace, var_names=["y_hat"], progressbar=False)
```
:::

## 

::: {.small}
```{python}
pm.summary(trace)
```
:::

##

::: {.small}
```{python}
plt.figure(layout="constrained")
plt.plot(x, pp.posterior_predictive['y_hat'].sel(chain=0).T, c="grey", alpha=0.01)
plt.scatter(x, y, s=30, label='data')
plt.show()
```
:::


## Demo 2 - Bayesian Lasso

```{python}
n = 50
k = 100

np.random.seed(1234)
X = np.random.normal(size=(n, k))

beta = np.zeros(shape=k)
beta[[10,30,50,70]] =  10
beta[[20,40,60,80]] = -10

y = X @ beta + np.random.normal(size=n)
```

::: {.aside}
Based on [Bayesian Sparse Regression](https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html) and [Lasso regression with block updating](https://docs.pymc.io/en/v3/pymc-examples/examples/pymc3_howto/lasso_block_update.html)
:::

## Naive model

::: {.small}
```{python naive}
#| cache: true
with pm.Model() as bayes_naive:
  b = pm.Flat("beta", shape=k)
  s = pm.HalfNormal('sigma', sigma=2)
  
  pm.Normal("y", mu=X @ b, sigma=s, observed=y)
  
  trace = pm.sample(progressbar=False, random_seed=12345)
```
:::

##

::: {.medium}
```{python naive summary}
#| cache: true
az.summary(trace)
```
:::

## Weakly informative model

::: {.small}
```{python weak}
#| cache: true
with pm.Model() as bayes_weak:
  b = pm.Normal("beta", mu=0, sigma=10, shape=k)
  s = pm.HalfNormal('sigma', sigma=2)
  
  pm.Normal("y", mu=X @ b, sigma=s, observed=y)
  
  trace = pm.sample(progressbar=False, random_seed=12345)
```
:::

##

::: {.medium}
```{python weak_summary}
#| cache: true
az.summary(trace)
```
:::

##

::: {.medium}
```{python weak_summary_hl}
#| cache: true
az.summary(trace).iloc[[10,20,30,40,50,60,70,80]]
```
:::

##

```{python weal_forest}
#| cache: true
ax = az.plot_forest(trace)
plt.tight_layout()
plt.show()
```

## Plot helper

::: {.small}
```{python}
def plot_slope(trace, prior="beta", chain=0):
  post = (trace.posterior[prior]
          .to_dataframe()
          .reset_index()
          .query(f"chain == {chain}")
         )
  
  sns.catplot(x="beta_dim_0", y="beta", data=post, kind="boxen", linewidth=0, color='blue', aspect=2, showfliers=False)
  plt.tight_layout()
  plt.xticks(range(0,110,10))
  plt.show()
  
```
:::

##

```{python weak_plot2}
#| cache: true
plot_slope(trace)
```


## Laplace Prior


::: {.small}
```{python laplace}
#| cache: true
with pm.Model() as bayes_lasso:
  b = pm.Laplace("beta", 0, 1, shape=k)
  s = pm.HalfNormal('sigma', sigma=1)
  
  pm.Normal("y", mu=X @ b, sigma=s, observed=y)
  
  trace = pm.sample(progressbar=False, random_seed=1234)
```
:::

##

::: {.medium}
```{python laplace_summary}
#| cache: true
az.summary(trace)
```
:::

##

::: {.medium}
```{python laplace_summary_hl}
#| cache: true
az.summary(trace).iloc[[10,20,30,40,50,60,70,80]]
```
:::

##

```{python laplace_plot}
#| cache: true
plot_slope(trace)
```